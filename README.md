# ğŸ§  Local RAG Assistant

The **Local RAG (Retrieval-Augmented Generation) Assistant** is a full-stack, privacy-friendly AI assistant that runs entirely on your machine. It allows users to upload files, perform semantic search over their content, and interact with a locally hosted LLM for contextual Q&A.

---

## ğŸ—‚ï¸ Project Structure

This project is divided into two parts:

## Local-RAG-Assistant/
# â”œâ”€â”€ rag-backend/     # Backend setup
# â”‚   â””â”€â”€ README.md    # ğŸ“˜ Backend setup and usage instructions
# â”œâ”€â”€ rag-frontend/    # Frontend: React + Vite + Tailwind CSS
# â”‚   â””â”€â”€ README.md    # ğŸ“˜ Frontend setup
# â””â”€â”€ README.md        # ğŸ“ This file (overall summary)

###  Clone the Repo

```bash
git clone https://github.com/Vikhram5/Local-RAG-Assistant.git
cd Local-RAG-Assistant
```

### Each folder includes its own `README.md` with detailed setup instructions, dependencies, and usage.
### Navigate to the `rag-backend` and `rag-frontend` respectively to run each of the files.

## âœ¨ Features

### ğŸ” Backend (Flask + LangChain + Ollama + Qdrant)
- ğŸ§  RAG pipeline with local LLM (LLaMA via Ollama)
- ğŸ“„ PDF/Text file ingestion and chunking
- ğŸ“š Embedding and vector search using Qdrant
- ğŸ”— LangChain integration for document retrievers and prompt chains
- ğŸ–¥ï¸ All components run locally â€“ No cloud dependency

### ğŸ–¥ï¸ Frontend (React + Vite)
- ğŸ“‚ File upload via Sidebar
- ğŸ’¬ Chat interface for Q&A
- âœ… React hooks for file and upload state
- ğŸ¨ Tailwind CSS for responsive design

---


