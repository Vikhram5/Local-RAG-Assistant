# ğŸ§  Local RAG Assistant

The **Local RAG (Retrieval-Augmented Generation) Assistant** is a full-stack, privacy-friendly AI assistant that runs entirely on your machine. It allows users to upload files, perform semantic search over their content, and interact with a locally hosted LLM for contextual Q&A.

---

###  Clone the Repo

```bash
git clone https://github.com/Vikhram5/Local-RAG-Assistant.git
cd Local-RAG-Assistant
```

### Each folder includes its own `README.md` with detailed setup instructions, dependencies, and usage.

## âœ¨ Features

### ğŸ” Backend (Flask + LangChain + Ollama + Qdrant)
- ğŸ§  RAG pipeline with local LLM (LLaMA via Ollama)
- ğŸ“„ PDF/Text file ingestion and chunking
- ğŸ“š Embedding and vector search using Qdrant
- ğŸ”— LangChain integration for document retrievers and prompt chains
- ğŸ–¥ï¸ All components run locally â€“ No cloud dependency

### ğŸ–¥ï¸ Frontend (React + Vite)
- ğŸ“‚ File upload via Sidebar
- ğŸ’¬ Chat interface for Q&A
- âœ… React hooks for file and upload state
- ğŸ¨ Tailwind CSS for responsive design

---


